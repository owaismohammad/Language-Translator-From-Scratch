# âœ¨ Attention is All You Need â€“ From Scratch + English to Kannada Translator (WIP)

Welcome to a personal deep dive into one of the most influential machine learning architectures of our time â€” the **Transformer**. This project is a full implementation of the seminal paper ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) â€” **from scratch** â€” using only Python and PyTorch.

Not just another copy-paste repo, this is a handcrafted, line-by-line reconstruction of the original transformer architecture, including:

- ğŸ”¸ Custom Multi-Head Attention  
- ğŸ”¸ Positional Encoding  
- ğŸ”¸ Layer Normalization  
- ğŸ”¸ Masking Strategies  
- ğŸ”¸ Feedforward Blocks  
- ğŸ”¸ Encoder and Decoder Stacks  
- ğŸ”¸ Training pipeline (data processing, batching)

All components have been built without relying on high-level libraries like `torch.nn.Transformer`. This was done to truly understand the **nuts and bolts** of how Transformers work under the hood.

---

## ğŸš€ Project Vision

I embarked on this project with one simple goal:

> **To implement a research paper from scratch and learn the architecture inside out.**

And what better candidate than the Transformer â€” the foundation for GPT, BERT, and every modern LLM today.

To apply the model in a meaningful way, I began building an **English to Kannada** translator using the architecture I implemented. This included writing:

- A custom tokenizer for both English and Kannada  
- Vocabulary builders  
- Data pipelines to prepare paired translations  
- Embedding layers integrated with positional encodings  
- A fully functional encoder-decoder setup with attention masking

---

## ğŸŒ English â¡ï¸ Kannada Translator (Work-in-Progress)

Building a translator was the ultimate test of everything implemented â€” integrating embedding layers, positional information, masking logic, attention across encoder-decoder stacks, and more.

However, due to **hardware constraints** (limited GPU memory and CPU power), full-scale training wasn't feasible. As a result, while the architecture is entirely in place and trains on toy datasets, real-world translation is not fully realized **yet**.

Nonetheless, the translator pipeline is complete in design, and ready to be scaled once sufficient compute is available.

---

## ğŸ§  Challenges Faced

Re-implementing a 2017 research paper with no black-box utilities was a challenge in itself. Highlights include:

- Wrangling tensor dimensions across multi-head attention and masking  
- Ensuring causal decoding with future-masked self-attention  
- Writing layer normalization from scratch with custom epsilon tweaks  
- Debugging vanishing gradients and memory bottlenecks  
- Designing an interface where `Sequential` layers could handle nested modules like attention blocks and multi-step decoder logic

These struggles, though intense, led to immense growth in:
- Deep **PyTorch proficiency**  
- Understanding of **sequence-to-sequence** modeling  
- Respect for the genius of the original architecture

---

## ğŸ› ï¸ Technologies Used

- ğŸ Python 3.10+  
- ğŸ”¥ PyTorch (Core APIs only â€” no high-level shortcuts!)  
- ğŸ§¾ JSONL + Custom Tokenizers  
- ğŸ“š Research paper: [Attention is All You Need](https://arxiv.org/abs/1706.03762)

---

## ğŸ“ Folder Structure

```bash
.
â”œâ”€â”€ data/                  # Parallel corpora for English-Kannada (or toy data)
â”œâ”€â”€ tokenizer/             # Custom tokenizer + vocabulary builders
â”œâ”€â”€ transformer/           # Attention, encoder, decoder, mask, normalization
â”œâ”€â”€ translator/            # Application logic for translation
â”œâ”€â”€ train.py               # Training loop
â”œâ”€â”€ utils.py               # Masking, batching, helpers
â””â”€â”€ README.md              # You're here!
```

---

## ğŸŒ± What's Next

- [ ] Improve batching and memory efficiency  
- [ ] Switch to mixed precision training (FP16)  
- [ ] Pretrain on larger English-Kannada datasets  
- [ ] Create a web interface for the translator  
- [ ] Optimize for CPU inference  

---

## ğŸ§‘â€ğŸ’» Final Words

This isn't just a machine learning project â€” it's a **journey into the heart of deep learning**, built one line of code at a time.

If you're someone who loves understanding things deeply, who wants to master PyTorch by implementing research ideas from scratch, or someone who just appreciates good engineering â€” I hope this project inspires you.

**Star** â­ the repo, fork it, study it, or reach out to collaborate.

Letâ€™s build great things â€” one head at a time.

---

**Made with patience, PyTorch, and purpose.**
